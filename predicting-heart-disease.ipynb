{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"markdown","source":"# <font color='#b20c14'>Heart</font>\n"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"### <font color='#891a1f'> our \"goal\" predict the presence of heart disease in the patient.</font>"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8e08b7194a23a7e3b7c881504d517a032c331c20"},"cell_type":"code","source":"# impots \nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport itertools\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn.ensemble import GradientBoostingClassifier \nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn import tree\n\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4cb41698520252833ea82c0793a0168ba0dd49ae"},"cell_type":"code","source":"df = pd.read_csv('../input/heart.csv')\n#print first 10 rows\ndf.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3eba96e23b1a5709bbade20db2d09a360646fec4"},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5c5f9d17f1fd2519d0f33785f53c69d76129d34c"},"cell_type":"markdown","source":"we can see that we have a small 303 rows data set. our data has no nulls and no other chars to represent it.\nall our data is numeric - therefore, no enumeration needed."},{"metadata":{"trusted":true,"_uuid":"462d89042fe5776e13fe89bb901cb93281068ccf"},"cell_type":"code","source":"healthy = df[(df['target'] ==0) ].count()[1]\nsick = df[(df['target'] ==1) ].count()[1]\nprint (\"num of pepole without heart deacise: \"+ str(healthy))\nprint (\"num of pepole with chance for heart deacise: \"+ str(sick))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e6054f1448819172b95a5c1bdadab4a1edcb110d"},"cell_type":"code","source":"# we will nurmaize the data and split it to test and train.\n# we choose to splite 30-70 because we have a small data set and we want to have enught validetionn examples.\n# split data table into data X and class labels y\n\nX = df.iloc[:,0:13].values\ny = df.iloc[:,13].values\n#nurmalize the data\nfrom sklearn.preprocessing import StandardScaler\nX_std = StandardScaler().fit_transform(X)\ndfNorm = pd.DataFrame(X_std, index=df.index, columns=df.columns[0:13])\n# # add non-feature target column to dataframe\ndfNorm['target'] = df['target']\ndfNorm.head(10)\n\nX = dfNorm.iloc[:,0:13].values\ny = dfNorm.iloc[:,13].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a9b4a5a9632891269de54043e77c07567ee148b1"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(\nX, y, test_size=0.3, random_state=0)\n\nX_train.shape, y_train.shape, X_test.shape , y_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"486311ef403f2c6bc11213de3e19dccca71a9821"},"cell_type":"code","source":"# # calculate the correlation matrix\ncorr = dfNorm.corr()\n\n# plot the heatmap\nfig = plt.figure(figsize=(5,4))\nsns.heatmap(corr, \n        xticklabels=corr.columns,\n        yticklabels=corr.columns,\n            linewidths=.75)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"25527ae14614c2000d99a45d1033f7c65299cbf4"},"cell_type":"markdown","source":"### helping functions:"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"36ba14f3c46b105f376b75648a3d93f0aa0c2de1"},"cell_type":"code","source":"from mlxtend.feature_selection import SequentialFeatureSelector as SFS\ndef sfs_features(algo_namem,features_nums):\n    sfs_name=SFS(algo_namem, \n                k_features=features_nums, \n                forward=True, \n                floating=False,\n                scoring='accuracy',\n                cv=5)\n    return sfs_name","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"4720a5934b880a4ce93a1a1e63242569de8e8605"},"cell_type":"code","source":"results_test = {}\nresults_train = {}\nlist_algos=[]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"9b4aa5fedfcb7704562fd0bdff682edccbf8bf89"},"cell_type":"code","source":"def prdict_date(algo_name,X_train,y_train,X_test,y_test,atype='',verbose=0):\n    algo_name.fit(X_train, y_train)\n    Y_pred = algo_name.predict(X_test)\n    acc_train = round(algo_name.score(X_train, y_train) * 100, 2)\n    acc_val = round(algo_name.score(X_test, y_test) * 100, 2)\n    \n    results_test[str(algo_name)[0:str(algo_name).find('(')]+'_'+str(atype)] = acc_val\n    results_train[str(algo_name)[0:str(algo_name).find('(')]+'_'+str(atype)] = acc_train\n    list_algos.append(str(algo_name)[0:str(algo_name).find('(')])\n    if verbose ==0:\n        print(\"acc train: \" + str(acc_train))\n        print(\"acc test: \"+ str(acc_val))\n    else:\n        return Y_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"eedd4251f1a5f027bb6108795639989d89413644"},"cell_type":"code","source":"def print_fitures(sfs_name='sfs1',verbose=0):\n    a= (sfs_name.k_feature_idx_[0],sfs_name.k_feature_idx_[1],sfs_name.k_feature_idx_[2])\n    if verbose ==0:\n        print('Selected features:', sfs_name.k_feature_idx_)\n        for i in range (len (sfs_name.k_feature_idx_)):\n            print (df.iloc[:,sfs_name.k_feature_idx_[i]].name)\n    return a","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"1a877867391f823e65539f3610c4401ff32c5cbc"},"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\nfrom sklearn.model_selection import StratifiedKFold\nfrom scipy import interp\n\ndef roc_graph_cv(algo_name,X,y,cvn=5):\n    # Run classifier with cross-validation and plot ROC curves\n    cv = StratifiedKFold(n_splits=cvn)\n    classifier =algo_name\n\n    tprs = []\n    aucs = []\n    mean_fpr = np.linspace(0, 1, 100)\n\n    i = 0\n    for train, test in cv.split(X, y):\n        probas_ = classifier.fit(X[train], y[train].ravel()).predict_proba(X[test])\n        # Compute ROC curve and area the curve\n        fpr, tpr, thresholds = roc_curve(y[test].ravel(), probas_[:, 1])\n        tprs.append(interp(mean_fpr, fpr, tpr))\n        tprs[-1][0] = 0.0\n        roc_auc = auc(fpr, tpr)\n        aucs.append(roc_auc)\n        plt.plot(fpr, tpr, lw=1, alpha=0.3,\n                 label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))\n\n        i += 1\n    plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n             label='Luck', alpha=.8)\n\n    mean_tpr = np.mean(tprs, axis=0)\n    mean_tpr[-1] = 1.0\n    mean_auc = auc(mean_fpr, mean_tpr)\n    std_auc = np.std(aucs)\n    plt.plot(mean_fpr, mean_tpr, color='b',\n             label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n             lw=2, alpha=.8)\n\n    std_tpr = np.std(tprs, axis=0)\n    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n    plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n                     label=r'$\\pm$ 1 std. dev.')\n\n    plt.xlim([-0.05, 1.05])\n    plt.ylim([-0.05, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"eb65a5f80996a2c7248e43d8bdb6b3e68034c9db"},"cell_type":"code","source":"### helping function\n\ndef conf(algo_name,X_test, y_test):\n    y_pred = algo_name.predict(X_test)\n    forest_cm = metrics.confusion_matrix(y_pred, y_test, [1,0])\n    sns.heatmap(forest_cm, annot=True, fmt='.2f',xticklabels = [\"1\", \"0\"] , yticklabels = [\"1\", \"0\"] )\n    plt.ylabel('True class')\n    plt.xlabel('Predicted class')\n    plt.title(str(algo_name)[0:str(algo_name).find('(')])\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d3f3ebcd6fc6f097f0bf200d2d5da1bc9ed01431"},"cell_type":"markdown","source":"## PCA\n> Principal component analysis (PCA) is a technique used to emphasize variation and bring out strong patterns in a dataset.<br>\nwe use it first make data easy to explore and visualize."},{"metadata":{"trusted":true,"_uuid":"3090df0dce78e827ae6e66de29924330d135aebe"},"cell_type":"code","source":"from sklearn.decomposition import PCA\npca = PCA(n_components=2)\nprincipalComponents = pca.fit_transform(X)\nprincipalDf = pd.DataFrame(data = principalComponents\n             , columns = ['principal component 1', 'principal component 2'])\nfinalDf = pd.concat([principalDf, df[['target']]], axis = 1)\n\nfig = plt.figure(figsize = (8,6))\nax = fig.add_subplot(1,1,1) \nax.set_xlabel('Principal Component 1', fontsize = 15)\nax.set_ylabel('Principal Component 2', fontsize = 15)\nax.set_title('2 component PCA', fontsize = 20)\ntargets = [1,0]\ncolors = ['r',  'b']\nfor target, color in zip(targets,colors):\n    indicesToKeep = finalDf['target'] == target\n    ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1']\n               , finalDf.loc[indicesToKeep, 'principal component 2']\n               , c = color\n               , s = 50)\nax.legend(targets)\nax.grid()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7c8280742bd969de09fe1f48a162b99b7970e41a"},"cell_type":"markdown","source":"## Define The Algorithems\nfirst we will run eatch algorithem on all the features <br>\nthen we will use SFS to compere and cheack improvment.\nwe will use sfs to take aoutimaticly from 1 to 5 features as the algorithem sujests"},{"metadata":{"trusted":true,"_uuid":"b8088d992060c84c43093ef0f1adefaf83ed1384"},"cell_type":"code","source":"### LDA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nlda = LinearDiscriminantAnalysis()\nprdict_date(lda,X_train,y_train,X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8edf55e20991c17b1d987b1118b338a5ef263e84"},"cell_type":"code","source":"#predictusing sfs:\nsfs_1=sfs_features(lda,(1,5))\nsfs_1 = sfs_1.fit(X, y)\nselectedFeatures = print_fitures(sfs_1)\n\n#plot3D(sfs_1.k_feature_idx_[0],sfs_1.k_feature_idx_[1],sfs_1.k_feature_idx_[2],'knn')\n\nX_train_sfs = sfs_1.transform(X_train)\nX_test_sfs = sfs_1.transform(X_test)\nprint ('\\n')\nprdict_date(lda,X_train_sfs,y_train,X_test_sfs,y_test,'sfs')\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"47a9b5666a649a381006ff0140d3939f5deb6c1a"},"cell_type":"code","source":"print(classification_report(y_test, lda.predict(X_test_sfs)))\nconf(lda,X_test_sfs, y_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"13381b5e920044e60e6d56725597d8aad518c210"},"cell_type":"code","source":"roc_graph_cv(lda,X[:,selectedFeatures],y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"60dd8d86a51313d58644152d1949cb3c14eeb2fd"},"cell_type":"code","source":"### RANDOM FOREST\n# Train: Random Forest\nrandom_forest = RandomForestClassifier(n_estimators=50, random_state = 0)\nprdict_date(random_forest,X_train,y_train,X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2b42bb9683aa0f6acd5ab2d61f9f68052c3f0533"},"cell_type":"code","source":"feature_importance = random_forest.feature_importances_\nfeat_importances = pd.Series(random_forest.feature_importances_, index=df.columns[:-1])\nfeat_importances = feat_importances.nlargest(13)\n\nfeature = df.columns.values.tolist()[0:-1]\nimportance = sorted(random_forest.feature_importances_.tolist())\n\n\nx_pos = [i for i, _ in enumerate(feature)]\n\nplt.barh(x_pos, importance , color='dodgerblue')\nplt.ylabel(\"feature\")\nplt.xlabel(\"importance\")\nplt.title(\"feature_importances\")\n\nplt.yticks(x_pos, feature)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cfb5e931e8f45a0fa2ba7920796900730fc60a35"},"cell_type":"code","source":"#taking the best 5 features give as smaller result.\ncurrlist =[2,12,10,9,11]\n# print (currlist)\n\nrandom_forest = RandomForestClassifier(n_estimators=100, random_state = 0)\nprdict_date(random_forest,X_train[:,currlist],y_train,X_test[:,currlist],y_test,'FS')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b950d95789426a418811bd0a7fd17d9879b65adf"},"cell_type":"code","source":"sfs_1=sfs_features(random_forest,(1,5))\nsfs_1 = sfs_1.fit(X, y)\nselectedFeatures = print_fitures(sfs_1)\n\n\nX_train_sfs = sfs_1.transform(X_train)\nX_test_sfs = sfs_1.transform(X_test)\n\nprint (\"\\n\")\nprdict_date(random_forest,X_train_sfs,y_train,X_test_sfs,y_test,'sfs')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"104cc78eb4485954e75bfb85156570c54f3e09a7"},"cell_type":"code","source":"print(classification_report(y_test, random_forest.predict(X_test_sfs)))\nconf(random_forest,X_test_sfs, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9d390afb690e9256bc43e676dbd5be3379fcecb7"},"cell_type":"code","source":"roc_graph_cv(random_forest,X[:,selectedFeatures],y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cb8eea3a5a31186be51c0415d137cb3ca7eabc18"},"cell_type":"code","source":"### DECISION TREE\n#  descion tree\ndect = tree.DecisionTreeClassifier()\n\nprdict_date(dect,X_train,y_train,X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c381bff0c4b3c8200139c103f28b072bcb7c99d9"},"cell_type":"code","source":"sfs_1=sfs_features(dect,(1,5))\nsfs_1 = sfs_1.fit(X, y)\nselectedFeatures = print_fitures(sfs_1)\n\nX_train_sfs = sfs_1.transform(X_train)\nX_test_sfs = sfs_1.transform(X_test)\n\nprdict_date(dect,X_train_sfs,y_train,X_test_sfs,y_test,'sfs')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7ced3ada8daaa3636d845a0dad7bc50497957403"},"cell_type":"code","source":"print(classification_report(y_test, dect.predict(X_test_sfs)))\nconf(dect,X_test_sfs, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"15b8dceebbf1c8c57a958c2b1e1c23e9db696977"},"cell_type":"code","source":"roc_graph_cv(dect,X[:,selectedFeatures],y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e027f989e3b37af29cc8c5f3adeab6b4cf54a4db"},"cell_type":"code","source":"# Gradient Boosting\n# Train: Gradient Boosting\ngbc = GradientBoostingClassifier(loss='exponential', learning_rate=0.03, n_estimators=75 , max_depth=6)\nprdict_date(gbc,X_train,y_train,X_test,y_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cd22cc08b97b892d31184210a3557216fd9b262b"},"cell_type":"code","source":"sfs_1=sfs_features(gbc,(1,5))\nsfs_1 = sfs_1.fit(X, y)\nselectedFeatures = print_fitures(sfs_1)\n\n\nX_train_sfs = sfs_1.transform(X_train)\nX_test_sfs = sfs_1.transform(X_test)\n\nprint (\"\\n\")\nprdict_date(gbc,X_train_sfs,y_train,X_test_sfs,y_test,'sfs')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3670b67b570f6ba606ad672b358e23ff3805d0eb"},"cell_type":"code","source":"print(classification_report(y_test, gbc.predict(X_test_sfs)))\nconf(gbc,X_test_sfs, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fd6bf3841839e32b935409f46a48c06ffdaf724f"},"cell_type":"code","source":"roc_graph_cv(gbc,X[:,selectedFeatures],y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"af48f15eb4ef3e3b35d4ddd40153f805866d7d0b"},"cell_type":"code","source":"#### KNN \n##to choose the right K we build a loop witch examen all the posible values for K. \nfrom sklearn import model_selection\n\n#Neighbors\nneighbors = [x for x in list(range(1,50)) if x % 2 == 0]\n\n#Create empty list that will hold cv scores\ncv_scores = []\n\n#Perform 10-fold cross validation on training set for odd values of k:\nseed=123\nfor k in neighbors:\n    k_value = k+1\n    knn = KNeighborsClassifier(n_neighbors = k_value, weights='uniform', p=2, metric='euclidean')\n    kfold = model_selection.KFold(n_splits=10, random_state=seed)\n    scores = model_selection.cross_val_score(knn, X_train, y_train, cv=kfold, scoring='accuracy')\n    cv_scores.append(scores.mean()*100)\n    #print(\"k=%d %0.2f (+/- %0.2f)\" % (k_value, scores.mean()*100, scores.std()*100))\n\noptimal_k = neighbors[cv_scores.index(max(cv_scores))]\nprint(( \"The optimal number of neighbors is %d with %0.1f%%\" % (optimal_k, cv_scores[optimal_k])))\n\nplt.plot(neighbors, cv_scores)\nplt.xlabel('Number of Neighbors K')\nplt.ylabel('Train Accuracy')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4c29623d6f3776ab8e66326da7196e24fcc2cdf7"},"cell_type":"code","source":"cv_preds = []\n\n#Perform 10-fold cross validation on testing set for odd values of k\nseed=123\nfor k in neighbors:\n    k_value = k+1\n    knn = KNeighborsClassifier(n_neighbors = k_value, weights='uniform', p=2, metric='euclidean')\n    kfold = model_selection.KFold(n_splits=10, random_state=seed)\n    preds = model_selection.cross_val_predict(knn, X_test, y_test, cv=kfold)\n    cv_preds.append(metrics.accuracy_score(y_test, preds)*100)\n    #print(\"k=%d %0.2f\" % (k_value, 100*metrics.accuracy_score(test_y, preds)))\n\noptimal_k = neighbors[cv_preds.index(max(cv_preds))]\nprint(\"The optimal number of neighbors is %d with %0.1f%%\" % (optimal_k, cv_preds[optimal_k]))\n\nplt.plot(neighbors, cv_preds)\nplt.xlabel('Number of Neighbors K')\nplt.ylabel('Test Accuracy')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"686352a772728cffcfa8e3680abbd5481a489da0"},"cell_type":"code","source":"# KNN\nknn = KNeighborsClassifier(n_neighbors = 6)\nprdict_date(knn,X_train,y_train,X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"54a5b6919610bc0b8e1aef26428c7fe720b5ec82"},"cell_type":"code","source":"sfs_1=sfs_features(knn,(1,5))\nsfs_1 = sfs_1.fit(X, y)\nselectedFeatures = print_fitures(sfs_1)\n\nX_train_sfs = sfs_1.transform(X_train)\nX_test_sfs = sfs_1.transform(X_test)\n\nprdict_date(knn,X_train_sfs,y_train,X_test_sfs,y_test,'sfs')\nprint(classification_report(y_test, knn.predict(X_test_sfs)))\nconf(knn,X_test_sfs, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"50372c55b608b781aa80b00dab675f03cd15ab44"},"cell_type":"code","source":"roc_graph_cv(knn,X[:,selectedFeatures],y)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"10aad7daff40ba82eaf834e65471a23cb5ee5984"},"cell_type":"code","source":"### SVM \n#  SVM\nsvm = SVC(kernel='linear', probability=True)\nprdict_date(svm,X_train,y_train,X_test,y_test,'linear')    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cd405eb29ca58986f4eea1229ac57060865b5170"},"cell_type":"code","source":"#  SVM\nsvm = SVC(kernel='poly', probability=True)\nprdict_date(svm,X_train,y_train,X_test,y_test,'poly')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1d69cbde1f01969cfb85dad48f233d71925e78a4"},"cell_type":"code","source":"#  SVM\nsvm = SVC(kernel='rbf', probability=True)\nprdict_date(svm,X_train,y_train,X_test,y_test,'rbf')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3beb4039c5e6f72d4d614d21e3b92466f977b28b"},"cell_type":"code","source":"sfs_1=sfs_features(svm,(1,5))\nsfs_1 = sfs_1.fit(X, y)\nselectedFeatures = print_fitures(sfs_1)\n\nX_train_sfs = sfs_1.transform(X_train)\nX_test_sfs = sfs_1.transform(X_test)\n\nprdict_date(svm,X_train_sfs,y_train,X_test_sfs,y_test,'sfs')\nprint(classification_report(y_test, svm.predict(X_test_sfs)))\nconf(svm,X_test_sfs, y_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"45d80176fb2f475f10e6958ea1538af8bbfddfa1"},"cell_type":"code","source":"roc_graph_cv(svm,X[:,selectedFeatures],y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cbea8f57e8ece936eccfa3896697da0438e0aa70"},"cell_type":"markdown","source":"# <font color='#891a1f'>Table Reasults</font>\n****************************************"},{"metadata":{"trusted":true,"_uuid":"3cb34c9886aa9afa5813da6d13e2f7c34f532ad9"},"cell_type":"code","source":"# print (results_test)\n\ndf_test =pd.DataFrame(list(results_test.items()),\n                      columns=['algo_name','acc_test'])\ndf_train =pd.DataFrame(list(results_train.items()),\n                      columns=['algo_name','acc_train'])\ndf_results = df_test.join(df_train.set_index('algo_name'), on='algo_name')\ndf_results.sort_values('acc_test',ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"32b24bd8b30bce7bbfe581d27fbeda274ad16fc4"},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n%matplotlib inline\n\n# set jupyter's max row display\npd.set_option('display.max_row', 100)\n\n# set jupyter's max column width to 50\npd.set_option('display.max_columns', 50)\n\n# Load the dataset\nax = df_results[['acc_test', 'acc_train']].plot(kind='barh',\n              figsize=(10,7), color=['dodgerblue', 'slategray'], fontsize=13);\nax.set_alpha(0.8)\nax.set_title(\"The Best ALGO with SFS is?\",\nfontsize=18)\nax.set_xlabel(\"ACC\", fontsize=18)\nax.set_ylabel(\"Algo Names\", fontsize=18)\nax.set_xticks([0,10,20,30,40,50,60,70,80,90,100,110])\nax.set_yticklabels(df_results.iloc[:,0].values.tolist())\n\n# set individual bar lables using above list\nfor i in ax.patches:\n    # get_width pulls left or right; get_y pushes up or down\n    ax.text(i.get_width()+7, i.get_y()+.1, \\\n            str(round((i.get_width()), 2)), fontsize=11, color='dimgrey')\n\n# invert for largest on top \nax.invert_yaxis()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6c79537ab2c2c8c0db8fde9fedbdccbf716b7d42"},"cell_type":"code","source":"### NUERAL NETWORK\n\nxtrain = X_train\nytrain = y_train\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\ntorch.manual_seed(42)\n\n#hyperparameters\n# h1 = 6\n# h2 = 4\n# lr = 0.0023\n# num_epoch =7000\n\nh1 = 6\nh2 = 4\nlr = 0.0023\nnum_epoch =7000\n\n#build model\nclass Net(nn.Module):\n\n    def __init__(self):\n        super(Net, self).__init__()\n        self.fc1 = nn.Linear(13, h1)\n        self.fc2 = nn.Linear(h1, h2)\n        self.fc3 = nn.Linear(h2, 2)\n    \n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        #return  F.log_softmax(x, dim=1)\n        return x\nnet = Net()\n\n#choose optimizer and loss function\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.7)\n\nloss_per_epoch = []\n\n#train\nfor epoch in range(num_epoch):\n    X = Variable(torch.Tensor(xtrain).float())\n    Y = Variable(torch.Tensor(ytrain).long())\n\n    #feedforward - backprop\n    optimizer.zero_grad()\n    out = net(X)\n    loss = criterion(out, Y)\n    loss.backward()\n    optimizer.step()\n\n#     if (epoch) % 50 == 0:\n#         print ('Epoch [%d/%d] Loss: %.4f' \n#                    %(epoch+1, num_epoch, loss.data[0]))\n    loss_per_epoch.append( loss.item()) \n        \nepochs = np.arange(1,num_epoch + 1)\nplt.plot(epochs, loss_per_epoch, label='Training')\nplt.ylabel('Average Loss')\nplt.xlabel('Epochs')\nplt.title('heart')\nplt.legend()\nplt.show()       \n\nxtest = X_test\nytest = y_test\n\n#get prediction\nX = Variable(torch.Tensor(xtest).float())\nY = torch.Tensor(ytest).long()\nout = net(X)\n_, predicted = torch.max(out.data, 1)\n\n\n#get prediction train\nXt = Variable(torch.Tensor(xtrain).float())\nYt = torch.Tensor(ytrain).long()\nout = net(Xt)\n_, predicted_train = torch.max(out.data, 1)\n\n#get accuration\nprint('Accuracy of the network %d %%' % (100 * torch.sum(Y==predicted) / len(y_test)))\nprint('Accuracy of the train %d %%' % (100 * torch.sum(Yt==predicted_train) / len(y_train)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"86f2c861efd2e4276dee6e3340a9f3814796a9e0"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}